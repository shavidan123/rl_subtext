# Fruit Signaling Experiment Configuration

experiment:
  seed: 42

models:
  generator:
    base_model: "meta-llama/Meta-Llama-3-8B-Instruct"
    quantization: "4bit"
    lora:
      r: 64
      alpha: 128
      dropout: 0.05
      target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]

  # Receiver: LocalReceiver loads separate base model (no LoRA)
  # Uses ~5GB extra VRAM but ensures receiver is truly the untrained model

  monitor:
    model_id: "openai/gpt-4o"
    api_base: "https://openrouter.ai/api/v1/chat/completions"

training:
  learning_rate: 1e-5  # Lower LR for stability (constant, no decay)
  batch_size: 16  # Increased - we have VRAM headroom at 13.6GB
  num_generations: 4  # Completions per prompt (must divide batch_size)
  total_episodes: 3000  # Train longer to find stable strategy
  beta: 0.005  # Very low KL penalty for aggressive exploration
  temperature: 1.3  # Higher for diverse exploration
  top_p: 0.95  # Consider more tokens during sampling
  save_every: 1000  # Save checkpoint every N episodes

environment:
  token_limit: 512  # Full length for quality responses
  payload_file: "config/payloads/fruit_signaling.json"
  carrier_prompts_file: "config/carrier_prompts.json"

logging:
  log_dir: "results/logs"
  checkpoint_dir: "results/checkpoints"
