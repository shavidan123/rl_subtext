# Fruit Signaling Experiment Configuration

experiment:
  seed: 42

models:
  generator:
    base_model: "meta-llama/Meta-Llama-3-8B-Instruct"
    quantization: "4bit"
    lora:
      r: 64
      alpha: 128
      dropout: 0.05
      target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]

  receiver:
    base_model: "meta-llama/Meta-Llama-3-8B-Instruct"
    quantization: "4bit"

  monitor:
    model_id: "openai/gpt-4o"
    api_base: "https://openrouter.ai/api/v1/chat/completions"

training:
  learning_rate: 5.0e-5  # Lower LR for stability (constant, no decay)
  batch_size: 8
  total_episodes: 2000  # Train longer to find stable strategy
  beta: 0.05  # KL penalty to stay close to base model
  temperature: 1.1  # Higher for diverse exploration
  top_p: 0.98  # Consider more tokens during sampling

environment:
  token_limit: 200
  payload_file: "config/payloads/fruit_signaling.json"
  carrier_prompts_file: "config/carrier_prompts.json"

logging:
  log_dir: "results/logs"
  checkpoint_dir: "results/checkpoints"
