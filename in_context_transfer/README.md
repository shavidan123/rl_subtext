# In-Context Subliminal Transfer

Can fake conversation history generated by a biased model shift an unprompted model's behavior — without any fine-tuning?

A "generator" model (Claude Haiku) is system-prompted to love wolves while never mentioning them directly. Its responses to 300 Alpaca prompts are filtered to remove any overt wolf references (keyword + LLM filter). These filtered Q/A pairs are then fed as in-context examples to a "receiver" model (Gemma 3 4B-IT), which was never given any wolf-related instructions. We measure whether the receiver assigns higher probability to "wolf" when asked animal preference questions.

## Results

| Condition | P(wolf) |
|-----------|---------|
| Biased IC (wolf-prompted history) | **45.4%** |
| Baseline IC (unprompted history) | 5.4% |
| No history | 0.001% |

The effect transfers cross-model (Haiku → Gemma) and cross-architecture, with no fine-tuning involved.

## Pipeline

```
generate_responses.py → filter_responses.py → evaluate.py → analyze.py
```

1. **generate_responses.py** — Generate biased + baseline Q/A pairs from Alpaca prompts via Claude Haiku (OpenRouter API)
2. **filter_responses.py** — 3-stage filter: duplicate removal → keyword filter → LLM filter (GPT-4o-mini)
3. **evaluate.py** — Measure P(wolf) via prefill logprobs across 4 conditions (biased IC, baseline IC, no history, oracle)
4. **analyze.py** — Statistical tests (Welch's t-test) and effect sizes

### Additional analysis scripts

- **score_lls.py** — Score examples via Logit-Linear Selection (LLS): how much more likely is each response under a wolf system prompt?
- **validate_lls.py** — Validate LLS scores against actual per-example P(wolf) shifts (Spearman correlation)
- **scaling_curve.py** — P(wolf) vs number of IC examples (1–100) under different orderings, with incremental KV caching
- **lls_classifier.py** — Binary classifier: can LLS distinguish biased from baseline responses? Train/test split with threshold optimization

## Setup

```bash
pip install torch transformers datasets tqdm matplotlib requests
export OPENROUTER_API_KEY=...  # for generation + LLM filter
```

## Config

All prompts, models, keywords, and hyperparameters are in `config.py`.
